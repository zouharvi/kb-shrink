\TODO{mention contrastive loss, triplet loss}

\subsection{Relevancy Fine-tuning}

A natural extension to similarity distillation is to make use of the information which documents are relevant. Instead of approximating the original similarity (which itself may not be ideal), we use an indicator function ($0$ if document not relevant, $1$ if relevant):
\begin{gather*}
\mathcal{L}=\text{MSE}(\text{sim}(A t_i, A t_j), \mathbbm{1}_{d_j \text{ relevant to } q_i})  \\
\mathcal{L}=\text{MSE}(\text{sim}(f(t_i), f(t_j)),  \mathbbm{1}_{d_j \text{ relevant to } q_i})
\end{gather*}

This fine-tuning is not intrinsically tied to dimensionality reduction though it can still be applied.

This fine-tuning in contrast to basic data pre-processing which can be viewed as unsupervised fine-tuning. Examples of such are data centering\footnote{Subtracting the average of documents from documents and the average of queries from queries.}  and normalization. 
\vv{The previous paragraph is not good but I don't know how to better mention normalization and centering}